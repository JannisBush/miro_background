<launch>

	<!-- The main monitor node. Responsible for the reactions and the monitoring -->
    <arg name="monitor_emotions" default="False" />
    <argm name="alert_faces" default="False" />
    <node name="monitor" pkg="miro_background" type="monitor_all.py" required="True">
    	<!-- Name of the robot. -->
    	<param name="robot_name" value="rob01" />
    	<!-- How many seconds the node should wait, before it starts to monitor/react -->
    	<param name="start_wait" value="5" />
    	<!-- How many seconds every interval (hour) should have  -->
    	<param name="interval_time" value="20" />
    	<!-- How many days a "week" consists of  -->
    	<param name="days" value="2" />
    	<!-- How many intervals (hours) a day consists of  -->
    	<param name="intervals_per_day" value="3" />
    	<!-- Should the robot display it's demo/autonomous behavior or just react to the stimuli without a lifelike feeling  -->
    	<param name="demo_mode" value="True" />
        <!-- Should the move options of the demo/autonomous behavior be activated? (even if it is False the robot will move) -->
        <param name="demo_move"  value="False" />
    	<!-- The base name and path of the monitor file  -->
    	<param name="monitor_file" value="$(find miro_background)/miro_monitor" />
    	<!-- The minimal confidence score that a keypoint of posenet gets accepted (important for HandsUp etc.)  -->
    	<param name="min_part_conf" value="0.4" />
        <!-- Should OpenFace Emotion Analysis be used? If it fails to install, set to false -->
        <param name="monitor_emotions" value="$(arg monitor_emotions)" />
	</node>


    <!-- The face safety node. Responsible for the detection of faces on the floor. Only start if the argument is set.-->
    <group if="$(arg alert_face)">
    	<node name="face_safety" pkg="miro_background" type="face_safety.py" args="robot=rob01" required="True"/>
    	<arg name="face_cascade_name_0"  default="$(find face_detection)/include/face_detection/HaarCascades/haarcascade_frontalface_alt.xml" />
    	<arg name="face_cascade_name_1"  default="$(find face_detection)/include/face_detection//HaarCascades/haarcascade_frontalface_alt2.xml" />
    	<arg name="face_cascade_name_2"  default="$(find face_detection)/include/face_detection//HaarCascades/haarcascade_frontalface_alt_tree.xml" />
    	<arg name="face_cascade_name_3"  default="$(find face_detection)/include/face_detection//HaarCascades/haarcascade_frontalface_default.xml" />
    	<arg name="face_cascade_name_4"  default="$(find face_detection)/include/face_detection/lbpCascades/lbpcascade_frontalface.xml" />
    	<node pkg="face_detection" type="face_detection_dlib" name="face_detection_dlib" required="True"
    	    args="$(arg face_cascade_name_0)
    	    $(arg face_cascade_name_1)
    	    $(arg face_cascade_name_2)
    	    $(arg face_cascade_name_3)
    	    $(arg face_cascade_name_4)"
    	    output="screen"/>
    </group>

	
	<!-- The client and server for the toy/object recognition. Specify the path to the grahp and the labels here. -->
    <node pkg="tensorflow_ros" type="object_recognition_node" name="toy_recognizer_server" args="_graph_path:=$(find image_recognition_util)/images/network/output_graph.pb _labels_path:=$(find image_recognition_util)/images/network/output_labels.txt" required="True" >
        <param name="topic_left" value="/camera/left/raw" />
        <param name="topic_right" value="/camera/right/raw" />
    </node>
    <node pkg="miro_background" type="recognize_objects.py" name="toy_recognizer_client" required="True" />


    <!-- The nodes for decompressing and republishing the camera streams -->
    <node pkg="image_transport" type="republish" name="republisherLeft" args="compressed in:=/miro/rob01/platform/caml raw out:=/camera/left/raw" required="True" />
    <node pkg="image_transport" type="republish" name="republisherRight" args="compressed in:=/miro/rob01/platform/camr raw out:=/camera/right/raw" required="True" />


    <!-- The node responsible for the Human Pose Recognition using Posenet. -->
    <node pkg="ros_posenet" type="posenet.js" name="posenet" output="screen">
    	<!-- The name of the image topic -->
        <param name="topic" value="/camera/left/raw" />
        <!-- Use the singlePerson classifier or the multiPerson classifier -->
        <param name="multiPerson" value="False" />
        <!-- Use of the gpu? @tensorflow/tfjs-node-gpu has to be installed -->
        <param name="gpu" value="false" />
        <!-- Name of the output topic -->
        <param name="poses_topic" value="/poses" />
        <!-- An optional number with values: `1.01`, `1.0`, `0.75`, or `0.50`. Defaults to `1.01`.   It is the float multiplier for the depth (number of channels) for all convolution operations. The value corresponds to a MobileNet architecture and checkpoint.  The larger the value, the larger the size of the layers, and more accurate the model at the cost of speed.  Set this to a smaller value to increase speed at the cost of accuracy. -->
        <param name="multiplier" value="1.01" />
        <!-- A number between 0.2 and 1.0. Defaults to 0.50. What to scale the image by before feeding it through the network. Set this number lower to scale down the image and increase the speed when feeding through the network at the cost of accuracy. -->
        <param name="image_scale_factor" value="0.5" />
        <!-- Defaults to false. If the poses should be flipped/mirrored horizontally. This should be set to true for videos where the video is by default flipped horizontally (i.e. a webcam), and you want the poses to be returned in the proper orientation. -->
        <param name="flip_horizontal" value="false" />
        <!-- The desired stride for the outputs when feeding the image through the model. Must be 32, 16, 8. Defaults to 16. The higher the number, the faster the performance but slower the accuracy, and visa versa. --> 
        <param name="output_stride" value="16" />
        <!-- The maximum number of poses to detect. Defaults to 5 -->
        <param name="max_pose" value="5" />
        <!-- Only return instance detections that have root part score greater or equal to this value. Defaults to 0.5 -->
        <param name="score_threshold" value="0.5" />
        <!-- Non-maximum suppression part distance. It needs to be strictly positive. Two parts suppress each other if they are less than nmsRadius pixels away. Defaults to 20. -->
        <param name="nms_radius" value="20" />
    </node>

    <!-- Only call OpenFace, if the flag was set -->
    <group if="$(arg monitor_emotions)">
        <!-- The node resposible for the Face Emotion Recogntion using OpenFace -->
    	<node pkg="features_face" type="executable_launcher_op.py" name="executable_launcher_op" required="True">
    		<!-- Location of OpenFace -->
      		<param name="pathOpenFace" value="$(find features_face)/OpenFace" /> 
      		<!-- The input image topic -->
      		<param name="image_topic" value="/image" />
      		<!-- Should the output be visualized? (at /openface/viz) -->
      		<param name="viz" value="False" />
        </node>
        <!-- Node to republish the camera stream in the correct way for OpenFace -->
        <node name="miro_camera" pkg="miro_background" type="miro_camera.py" args="robot=rob01" required="True">
    	  		<param name="pathToYAMLCamCalFile" value="$(find miro_background)/ost.yaml" /> 
    	</node>
    </group>

</launch>